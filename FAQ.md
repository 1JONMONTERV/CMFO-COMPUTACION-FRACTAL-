# CMFO Frequently Asked Questions

> **Foreword:** We answer the hard questions directly. No evasion.

## 1. Is this a Large Language Model (LLM)?
**No.** CMFO is not a Transformer. It does not use Attention Heads or Backpropagation. It is a **Fractal Geometric Kernel**. It can perform language tasks, but the underlying mechanism is resonance, not statistical prediction.

## 2. Where is the training data?
CMFO is not "trained" on petabytes of internet text. It is **calibrated** on structural patterns. Just as you don't "train" a calculator to add numbers (you program the logic), CMFO is pre-programmed with the logic of language geometry.

## 3. Does it hallucinate?
**No.** Hallucination is a feature of probabilistic models guessing the next word. CMFO is deterministic. If it doesn't know the answer, it converges to a "Null Attractor" (silence/uncertainty) rather than inventing facts.

## 4. Why 7 Dimensions?
This is not arbitrary. It derives from the **Octonion Fano Plane** structures and the optimal packing of information in high-dimensional space related to the Golden Ratio ($\varphi$). It is the minimum dimension required for self-referential closure.

## 5. Can I run this on my bespoke FPGA?
Yes. Unlike Neural Networks that require massive float32 matrix multipliers (MatMul), CMFO relies on fractal recursion, which is highly potentially efficient on FPGA hardware. We are working on a Verilog backend.
